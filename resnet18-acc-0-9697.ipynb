{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd537c4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:46:03.149200Z",
     "iopub.status.busy": "2025-05-03T10:46:03.148893Z",
     "iopub.status.idle": "2025-05-03T10:46:13.355489Z",
     "shell.execute_reply": "2025-05-03T10:46:13.354885Z"
    },
    "papermill": {
     "duration": 10.211885,
     "end_time": "2025-05-03T10:46:13.356923",
     "exception": false,
     "start_time": "2025-05-03T10:46:03.145038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fe43bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:46:13.362037Z",
     "iopub.status.busy": "2025-05-03T10:46:13.361727Z",
     "iopub.status.idle": "2025-05-03T10:46:13.445014Z",
     "shell.execute_reply": "2025-05-03T10:46:13.444438Z"
    },
    "papermill": {
     "duration": 0.086883,
     "end_time": "2025-05-03T10:46:13.446232",
     "exception": false,
     "start_time": "2025-05-03T10:46:13.359349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_value = 42\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56be12b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:46:13.450981Z",
     "iopub.status.busy": "2025-05-03T10:46:13.450760Z",
     "iopub.status.idle": "2025-05-03T10:46:24.458089Z",
     "shell.execute_reply": "2025-05-03T10:46:24.457345Z"
    },
    "papermill": {
     "duration": 11.011322,
     "end_time": "2025-05-03T10:46:24.459588",
     "exception": false,
     "start_time": "2025-05-03T10:46:13.448266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: Counter({0: 541, 1: 110})\n"
     ]
    }
   ],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for cls_name in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls_name)\n",
    "            for img_name in os.listdir(cls_path):\n",
    "                img_path = os.path.join(cls_path, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(self.class_to_idx[cls_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "data_dir = '/kaggle/input/test-dataset/Fire-Detection' \n",
    "\n",
    "image_size = 224\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "full_dataset = CustomImageDataset(root_dir=data_dir, transform=transform)\n",
    "\n",
    "labels = [label for _, label in full_dataset]\n",
    "class_counts = Counter(labels)\n",
    "print('Class Distribution:', class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb338798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:46:24.467209Z",
     "iopub.status.busy": "2025-05-03T10:46:24.466963Z",
     "iopub.status.idle": "2025-05-03T10:46:24.481878Z",
     "shell.execute_reply": "2025-05-03T10:46:24.481017Z"
    },
    "papermill": {
     "duration": 0.019995,
     "end_time": "2025-05-03T10:46:24.483093",
     "exception": false,
     "start_time": "2025-05-03T10:46:24.463098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution of Training Set: Counter({0: 432, 1: 88})\n",
      "Validation Set Class Distribution: Counter({0: 54, 1: 11})\n",
      "Test Set Class Distribution: Counter({0: 55, 1: 11})\n",
      "\n",
      "DataLoaders created.\n",
      "Training DataLoader size: 17\n",
      "Validation DataLoader size: 3\n",
      "Test DataLoader size: 3\n"
     ]
    }
   ],
   "source": [
    "train_idx, remaining_idx = train_test_split(\n",
    "    range(len(full_dataset)),\n",
    "    test_size=0.2,\n",
    "    stratify=full_dataset.labels,\n",
    "    random_state=42  \n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    remaining_idx,\n",
    "    test_size=0.5,\n",
    "    stratify=[full_dataset.labels[i] for i in remaining_idx],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "print('Class Distribution of Training Set:', Counter([full_dataset.labels[i] for i in train_idx]))\n",
    "print('Validation Set Class Distribution:', Counter([full_dataset.labels[i] for i in val_idx]))\n",
    "print('Test Set Class Distribution:', Counter([full_dataset.labels[i] for i in test_idx]))\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('\\nDataLoaders created.')\n",
    "print('Training DataLoader size:', len(train_loader))\n",
    "print('Validation DataLoader size:', len(val_loader))\n",
    "print('Test DataLoader size:', len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8fb44b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:46:24.488545Z",
     "iopub.status.busy": "2025-05-03T10:46:24.487990Z",
     "iopub.status.idle": "2025-05-03T10:46:25.372554Z",
     "shell.execute_reply": "2025-05-03T10:46:25.371686Z"
    },
    "papermill": {
     "duration": 0.888728,
     "end_time": "2025-05-03T10:46:25.374001",
     "exception": false,
     "start_time": "2025-05-03T10:46:24.485273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 151MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained ResNet18 model was loaded and its last layer was updated.\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print('The pre-trained ResNet18 model was loaded and its last layer was updated.')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f97c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:46:25.379904Z",
     "iopub.status.busy": "2025-05-03T10:46:25.379700Z",
     "iopub.status.idle": "2025-05-03T10:46:25.384068Z",
     "shell.execute_reply": "2025-05-03T10:46:25.383391Z"
    },
    "papermill": {
     "duration": 0.008391,
     "end_time": "2025-05-03T10:46:25.385071",
     "exception": false,
     "start_time": "2025-05-03T10:46:25.376680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss function and optimization algorithm are defined.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.001  \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print('The loss function and optimization algorithm are defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0d03af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:46:25.390713Z",
     "iopub.status.busy": "2025-05-03T10:46:25.390455Z",
     "iopub.status.idle": "2025-05-03T10:48:31.486648Z",
     "shell.execute_reply": "2025-05-03T10:48:31.485835Z"
    },
    "papermill": {
     "duration": 126.100482,
     "end_time": "2025-05-03T10:48:31.487966",
     "exception": false,
     "start_time": "2025-05-03T10:46:25.387484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss of Train: 0.2714, Validation Loss:: 3.4018, Validation Accuracy: 0.6769\n",
      "Epoch [2/20], Loss of Train: 0.1415, Validation Loss:: 0.1272, Validation Accuracy: 0.9692\n",
      "Epoch [3/20], Loss of Train: 0.0961, Validation Loss:: 0.2138, Validation Accuracy: 0.9538\n",
      "Epoch [4/20], Loss of Train: 0.0624, Validation Loss:: 0.2293, Validation Accuracy: 0.9231\n",
      "Epoch [5/20], Loss of Train: 0.0422, Validation Loss:: 0.1347, Validation Accuracy: 0.9385\n",
      "Epoch [6/20], Loss of Train: 0.0563, Validation Loss:: 0.1210, Validation Accuracy: 0.9692\n",
      "Epoch [7/20], Loss of Train: 0.0439, Validation Loss:: 0.2288, Validation Accuracy: 0.9385\n",
      "Epoch [8/20], Loss of Train: 0.0629, Validation Loss:: 0.2061, Validation Accuracy: 0.9538\n",
      "Epoch [9/20], Loss of Train: 0.0982, Validation Loss:: 0.1208, Validation Accuracy: 0.9538\n",
      "Epoch [10/20], Loss of Train: 0.0557, Validation Loss:: 0.3949, Validation Accuracy: 0.8769\n",
      "Epoch [11/20], Loss of Train: 0.0489, Validation Loss:: 0.2544, Validation Accuracy: 0.9538\n",
      "Epoch [12/20], Loss of Train: 0.0729, Validation Loss:: 0.2469, Validation Accuracy: 0.9538\n",
      "Epoch [13/20], Loss of Train: 0.0905, Validation Loss:: 0.1521, Validation Accuracy: 0.9385\n",
      "Epoch [14/20], Loss of Train: 0.0594, Validation Loss:: 0.1245, Validation Accuracy: 0.9538\n",
      "Early stopping applied! Best validation loss: 0.1208\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1) \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs[:, 0].unsqueeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "\n",
    "        model.eval()  \n",
    "        val_loss = 0.0\n",
    "        correct_predictions_val = 0\n",
    "        total_samples_val = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs[:, 0].unsqueeze(1)\n",
    "                val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "                predictions = (probabilities > 0.5).float()\n",
    "                correct_predictions_val += (predictions == labels).sum().item()\n",
    "                total_samples_val += labels.size(0)\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_val_accuracy = correct_predictions_val / total_samples_val\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_accuracy'].append(epoch_val_accuracy)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss of Train: {epoch_loss:.4f}, Validation Loss:: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_accuracy:.4f}')\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), '/kaggle/working/best_model.pth') \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f'Early stopping applied! Best validation loss: {best_val_loss:.4f}')\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n",
    "    return model, history\n",
    "\n",
    "num_epochs = 20  \n",
    "patience = 5    \n",
    "trained_model, history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618ee41a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:48:31.495223Z",
     "iopub.status.busy": "2025-05-03T10:48:31.494741Z",
     "iopub.status.idle": "2025-05-03T10:48:32.217341Z",
     "shell.execute_reply": "2025-05-03T10:48:32.216371Z"
    },
    "papermill": {
     "duration": 0.727619,
     "end_time": "2025-05-03T10:48:32.218747",
     "exception": false,
     "start_time": "2025-05-03T10:48:31.491128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1035, Test Accuracy: 0.9697\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Fire       1.00      0.96      0.98        55\n",
      "        Fire       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.97        66\n",
      "   macro avg       0.92      0.98      0.95        66\n",
      "weighted avg       0.97      0.97      0.97        66\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[53  2]\n",
      " [ 0 11]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  \n",
    "    test_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs[:, 0].unsqueeze(1)\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(all_labels, all_predictions, target_names=['No Fire', 'Fire']))\n",
    "\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "evaluate_model(trained_model, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 399530,
     "sourceId": 767109,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 155.282979,
   "end_time": "2025-05-03T10:48:34.344039",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-03T10:45:59.061060",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
