{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_value = 42 \n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for cls_name in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls_name)\n",
    "            for img_name in os.listdir(cls_path):\n",
    "                img_path = os.path.join(cls_path, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(self.class_to_idx[cls_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]   \n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        #print(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: Counter({1: 1333, 0: 1018})\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"C:\\Users\\Gell G15\\OneDrive\\Desktop\\Cognitive project -fire detection\\Fire_detection_set\"\n",
    "\n",
    "image_size = 224\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "full_dataset = CustomImageDataset(root_dir=data_dir, transform=transform)\n",
    "\n",
    "labels = [label for _, label in full_dataset]\n",
    "class_counts = Counter(labels)\n",
    "print('Class Distribution:', class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution of Training Set: Counter({1: 1056, 0: 824})\n",
      "Validation Set Class Distribution: Counter({1: 134, 0: 101})\n",
      "Test Set Class Distribution: Counter({1: 143, 0: 93})\n"
     ]
    }
   ],
   "source": [
    "# Consider using torch's random_split for cleaner code\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "# Use random_split with generator for reproducibility\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_subset, validation_subset, test_subset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "#Create DataLoaders with consistent naming\n",
    "batch_size = 32\n",
    "train_data_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "validation_data_loader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Class Distribution of Training Set:', Counter([full_dataset.labels[i] for i in train_subset.indices]))\n",
    "print('Validation Set Class Distribution:', Counter([full_dataset.labels[i] for i in validation_subset.indices]))\n",
    "print('Test Set Class Distribution:', Counter([full_dataset.labels[i] for i in test_subset.indices]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gell G15\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gell G15\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained ResNet18 model was loaded and its last layer was updated.\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"from PIL import Image\\nimport torch\\nimport torchvision.transforms as transforms\\n\\ndef predict_image(image_path, model, transform, device):\\n    image = Image.open(image_path).convert('RGB')\\n    image = transform(image).unsqueeze(0).to(device)\\n\\n    model.eval()\\n    with torch.no_grad():\\n        outputs = model(image)\\n        _, predicted = torch.max(outputs, 1)\\n        confidence = torch.nn.functional.softmax(outputs, dim=1)[0][predicted].item()\\n        \\n    return predicted.item(), confidence\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)  # Changed to 1 output for binary classification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print('The pre-trained ResNet18 model was loaded and its last layer was updated.')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss function and optimization algorithm are defined.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()  # Loss Function: Binary Cross Entropy with Logits\n",
    "learning_rate = 0.001  \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print('The loss function and optimization algorithm are defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.0643, Train Accuracy: 0.9798, Validation Loss: 0.1568, Validation Accuracy: 0.9660\n",
      "Epoch [2/20], Train Loss: 0.0568, Train Accuracy: 0.9777, Validation Loss: 0.1441, Validation Accuracy: 0.9617\n",
      "Epoch [3/20], Train Loss: 0.0586, Train Accuracy: 0.9819, Validation Loss: 0.1045, Validation Accuracy: 0.9617\n",
      "Epoch [4/20], Train Loss: 0.0467, Train Accuracy: 0.9856, Validation Loss: 0.1103, Validation Accuracy: 0.9617\n",
      "Epoch [5/20], Train Loss: 0.0325, Train Accuracy: 0.9899, Validation Loss: 0.3282, Validation Accuracy: 0.9277\n",
      "Epoch [6/20], Train Loss: 0.0389, Train Accuracy: 0.9851, Validation Loss: 0.1540, Validation Accuracy: 0.9362\n",
      "Epoch [7/20], Train Loss: 0.0305, Train Accuracy: 0.9888, Validation Loss: 0.0616, Validation Accuracy: 0.9830\n",
      "Epoch [8/20], Train Loss: 0.0276, Train Accuracy: 0.9888, Validation Loss: 0.1823, Validation Accuracy: 0.9702\n",
      "Epoch [9/20], Train Loss: 0.0231, Train Accuracy: 0.9910, Validation Loss: 0.1445, Validation Accuracy: 0.9745\n",
      "Epoch [10/20], Train Loss: 0.0140, Train Accuracy: 0.9952, Validation Loss: 0.1743, Validation Accuracy: 0.9660\n",
      "Epoch [11/20], Train Loss: 0.0136, Train Accuracy: 0.9947, Validation Loss: 0.1361, Validation Accuracy: 0.9532\n",
      "Epoch [12/20], Train Loss: 0.0058, Train Accuracy: 0.9979, Validation Loss: 0.1220, Validation Accuracy: 0.9702\n",
      "Early stopping applied! Best validation loss: 0.0616\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=5):\n",
    "    best_val_loss = float('inf')  # Keeps track of best validation loss\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1) \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_predictions / total_samples\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "\n",
    "        model.eval()  \n",
    "        val_loss = 0.0\n",
    "        correct_predictions_val = 0\n",
    "        total_samples_val = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "                predictions = (probabilities > 0.5).float()\n",
    "                correct_predictions_val += (predictions == labels).sum().item()\n",
    "                total_samples_val += labels.size(0)\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_val_accuracy = correct_predictions_val / total_samples_val\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_accuracy'].append(epoch_val_accuracy)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_accuracy:.4f}')\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'Fire_detection_model.pth') \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f'Early stopping applied! Best validation loss: {best_val_loss:.4f}')\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load('Fire_detection_model.pth'))\n",
    "    return model, history\n",
    "\n",
    "num_epochs = 20  \n",
    "patience = 5    \n",
    "trained_model, history = train_model(model, train_data_loader, validation_data_loader, criterion, optimizer, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0727\n",
      "Test Accuracy: 0.9661\n",
      "Precision: 0.9530\n",
      "Recall: 0.9930\n",
      "F1-Score: 0.9726\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fire       0.99      0.92      0.96        93\n",
      "     No Fire       0.95      0.99      0.97       143\n",
      "\n",
      "    accuracy                           0.97       236\n",
      "   macro avg       0.97      0.96      0.96       236\n",
      "weighted avg       0.97      0.97      0.97       236\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  \n",
    "    test_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            \n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
    "\n",
    "    #_______________________________________________Calculate metrics_________________________________________________________\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "\n",
    "    #_______________________________________________Print results_________________________________________________________\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')\n",
    "    \n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(all_labels, all_predictions, target_names=['Fire', 'No Fire']))\n",
    "\n",
    "    #print('\\nConfusion Matrix:')\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    #print(cm)\n",
    "\n",
    "    #__________________________________________________PLOT CONFUSION MATRIX_______________________________________________________\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Fire', 'No Fire'], \n",
    "                yticklabels=['Fire', 'No Fire'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    return all_probabilities, all_labels, all_predictions\n",
    "\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "test_probabilities, test_labels, test_predictions = evaluate_model(trained_model, test_data_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model architecture: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model - CORRECTED VERSION\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)  # Changed to 1 output neuron for binary classification\n",
    "model.load_state_dict(torch.load('Fire_detection_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print('Model loaded successfully!')\n",
    "print(f'Model architecture: {model}')\n",
    "print(f'Model device: {next(model.parameters()).device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: No Fire (1.0)\n",
      "Confidence: 0.00%\n",
      "✅ No fire detected.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have the predict_image function defined\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def predict_image(image_path, model, transform, device):\n",
    "    try:\n",
    "        # Open and preprocess the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image)\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predicted = (probabilities > 0.5).float()\n",
    "            # Confidence calculation adjusted for inverted labels\n",
    "            confidence = probabilities.item() if predicted.item() == 0 else 1 - probabilities.item()\n",
    "            \n",
    "        return predicted.item(), confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting image: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Define your transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Example with a specific image\n",
    "image_path = r\"C:\\Users\\Gell G15\\OneDrive\\Desktop\\Cognitive project -fire detection\\Fire_detection_set\\Neutral\\6.jpg\"\n",
    "\n",
    "# Make prediction\n",
    "predicted_class, confidence = predict_image(image_path, model, transform, device)\n",
    "\n",
    "if predicted_class is not None:\n",
    "    class_names = ['Fire', 'No Fire']  # 0 = Fire, 1 = No Fire\n",
    "    print(f\"Predicted Class: {class_names[int(predicted_class)]} ({predicted_class})\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")\n",
    "    \n",
    "    # Additional interpretation\n",
    "    if predicted_class == 0:\n",
    "        print(\"🚨 Fire detected!\")\n",
    "    else:\n",
    "        print(\"✅ No fire detected.\")\n",
    "else:\n",
    "    print(\"Prediction failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
